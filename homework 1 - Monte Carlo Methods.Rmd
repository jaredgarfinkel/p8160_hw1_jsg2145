---
title: "Homework 1 - Monte Carlo Methods"
author: "Jared Garfinkel - jsg2145"

output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1
The standard Laplace distribution has density $$f(x) = 0.5e^{-|x|}, x \in
(-\infty, \infty)$$. Please provide an algorithm that uses the inverse transformation method to generate a random sample from this distribution. Use the $U(0,1)$ random number generator in  \em{\bf{R}}, write a  \em{\bf{R}}-function to implement the algorithm. Use visualization tools to validate your algorithm (i.e., illustrate whether the random numbers generated from your function truely follows the standard Laplace distribution.)

# Answer: your answer starts here...

X = $F^{-1}(U)$

\mu = $F(x)$

$f(x) =  0.5e^{-|x|},~x \in(-\infty, \infty)$

\begin{document}
\[   \left\{
\begin{array}{ll}
      F(x) = 0.5e^x & x \le 0 \\
      F(x) = 1-0.5e^{-x} & x > 0\\
\end{array} 
\right. \]
\end{document}

\begin{document}
\[   \left\{
\begin{array}{ll}
      x = ln(2U) & U < \frac{1}{2} \\
      x = -ln(2-2U) & U \ge \frac{1}{2}\\
\end{array} 
\right. \]
\end{document}

```{r }
set.seed(123)
U <- runif(1000)
X <- (U < 0.5) * log(2*U) + (U >= 0.5) * -log(2-2*U)

plot(X)
hist(X, prob = T)


```

#Problem 2

Use the inverse transformation method to  derive an algorithm for generating a Pareto random number with $U\sim U(0,1)$, where the Pareto random number has a probability density function 
     $$f(x; \alpha, \gamma)=\frac{\gamma\alpha^{\gamma}}{x^{\gamma+1}} I\{x\ge \alpha\}$$ with two parameters $\alpha >0$ and $\gamma>0$. Use visualization tools to validate your algorithm (i.e., illustrate whether the random numbers generated from your function truely follows the target distribution.)

$$F(x) = 1 - \left(\frac{\alpha}{x}\right)^{\gamma},~x \ge \alpha,~\alpha > 0,~\gamma > 0$$

$$x = \frac{\alpha}{(1-u)^{1/\gamma}}$$

# Answer:  your answer starts here...
```{r }
set.seed(1001)
U <- runif(1000)
xdens = function(gamma = 5, alpha = 2, x = U) {
  alpha/((1-U)^(1/gamma))
}

x <- xdens(5, 2, U)

hist(x, prob = T)
```

#Problem 3

Construct an algorithm for using the acceptance/rejection method to generate 100 pseudorandom
 variable from the pdf
 $$f(x) = \frac{2}{\pi \beta^2} \sqrt{\beta^2-x^2}, \,\, -\beta \le x \le \beta.$$
The simplest choice for $g(x)$ is the $U(-\beta, \beta)$ distribution but other choices are possible as well. Use visualization tools to validate your algorithm (i.e., illustrate whether the random numbers generated from your function truely follows the target distribution.)

# Answer:  your answer starts here...

```{r}
set.seed(1001)

accrej <- function(fdens, gdens, beta, M = (4/pi), x){
  x = runif(2222, min = -beta, max = beta)
  return(x[runif(length(x)) <= fdens(x, beta) / (M * gdens(x, beta))])
}

xdens = function(x, beta){
  return((2/(pi*beta^2))*sqrt(beta^2 - x^2) * (abs(x) <= beta))
}

unifdens = function(x, beta){
  return((1/(2*beta))*(abs(x) <= beta))
}


y = accrej(xdens, unifdens, 3, 4/pi)

hist(y, prob = T)
```

#Problem 4

Develop two Monte Carlo methods for the estimation of $\theta=\int_0^1 e^{x^2} dx$ and implement in \em{\bf{R}} .

# Answer:  your answer starts here...
```{r exponential data}
n = 10000
u = runif(n)
y = sum(exp(u^2))/n
x = median(exp(u^2))

xy = tibble(
  median = x,
  mean = y
)
```

```{r, include = FALSE, eval = FALSE}
compare <- function(n, x, N=500) {
  SSEmean <- SSEmedian <- 0                    
# Initialize
  for(i in 1:N){
    dat <- integrate(simdat, 0, 1)
    SSEmean <- SSEmean + mean(dat)^2
    SSEmedian <- SSEmedian + median(dat)^2
  }
  
  return(list(n=n, x=x, MSEmean = SSEmean / N,
              MSEmedian = SSEmedian / N))
}
```

```{r, include = FALSE, eval = FALSE}
pvec <- seq(1, 30, by=1)

res <- NULL

for(i in 1:length(pvec)){
  res <- rbind(res, as.numeric(compare(30, pvec[i])))
}

res <- data.frame(res)
names(res) <- c("n", "x", "MSEmean", "MSEmedian")

plot(res$x, res$MSEmean, type="l", xlab="x", ylab="MSE",
     ylim=c(0,max(res$MSEmean)))
lines(res$x, res$MSEmedian,lty=2)
```



#Problem 5
Show that in estimating $\theta=E\sqrt{1-U^2}$ it is better to use $U^2$ rather than $U$ as the control variate, where $U\sim U(0,1)$.  To do this, use simulation to approximate the necessary covariances. In addition, implement your algorithms in  \em{\bf{R}}.

# Answer:  your answer starts here...

```{r }
gfun<-function(x){ 
  sqrt(1 - x^2)
}
mfun<-function(x) {
  x^2
}
mfun2<-function(x) {
  x
}
set.seed(123)
uran<-runif(10000)
ga<-gfun(uran)
ma<-mfun(uran)
ma2<-mfun2(uran)
```

```{r}
theta1<-mean(ga)
theta2<-mean(ma2)
hha<- pi/4 + (ga-ma)
hha2<- pi/4 + (ga-ma2)
theta1a<-mean(hha)
theta2a<-mean(hha2)

c(var(ga), var(hha))

c(var(ga), var(hha2))


(var(ga)-var(hha))/var(ga)

(var(ga)-var(hha2))/var(ga)
```


#Problem 6
Obtain a Monte Carlo estimate of
$$\int_1^\infty \frac{x^2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$$
by importance sampling and evaluate its variance. Write a  \em{\bf{R}} function to implement your procedure.

## Use a Cauchy distribution to implement importance sampling.

```{r }
ncandidates <- 1000;  
M <- sqrt(2*pi/exp(1))
u <- runif(ncandidates)
x <- tan(pi * (runif(ncandidates) - 0.5))
accepted <- NULL      # Initialize the vector of accepted values
for(i in 1:ncandidates)
  if(u[i] <= dnorm(x[i])/(M * dcauchy(x[i])))
    accepted <- c(accepted, x[i])    # Accept x[i]
accepted <- x[u <= dnorm(x)/(M * dcauchy(x))]
c(mean(accepted), sd(accepted))
```


